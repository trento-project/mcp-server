// Copyright 2025 SUSE LLC
// SPDX-License-Identifier: Apache-2.0

== SUSE AI Integration

This document provides a guide on how to use the `Trento MCP Server` with SUSE AI, including configuration and deployment instructions.

=== Getting Started with SUSE AI

SUSE AI is a platform that allows you to deploy and manage AI models and applications in a Kubernetes environment. It provides tools for model management, deployment, and integration with various AI frameworks.

Refer to the official https://documentation.suse.com/suse-ai/1.0/[SUSE AI documentation] for detailed information. In this guide, we will focus on deploying the `Trento MCP Server` using SUSE AI, specifically with Ollama and Open Web UI. Always refer to the latest instructions in the https://documentation.suse.com/suse-ai/1.0/html/AI-deployment-intro/index.html[SUSE AI deployment guide] for the most accurate and up-to-date information.

=== Prerequisites

This guide assumes:

* you have a Kubernetes cluster set up and running (with an ingress controller and cert-manager installed)
* you have access to the SUSE Application Collection.
* To run Ollama models, you have either:
** a Kubernetes cluster with sufficient resources.
** a cloud provider and enough permissions to deploy Ollama models remotely.

==== Limitations

Deploying the entire SUSE AI stack requires plenty of resources, especially for running Ollama models. Alternatively, you can deploy only the Open Web UI and connect it to an Ollama instance running elsewhere. This is the approach we will take here: on-premises Open Web UI with Ollama running on a remote server (e.g., Google Cloud).

==== Getting the artifacts from SUSE Application Collection

You need access to SUSE Application Collection and the proper entitlements to download the required artifacts. Always refer to the https://docs.apps.rancher.io/get-started/authentication/[SUSE Application Collection documentation] for the latest instructions on how to authenticate and access the collection. This guide assumes you have the necessary credentials and access to the collection.

____
NOTE:To run the entire stack, you can also use the https://github.com/SUSE/suse-ai-deployer[SUSE AI Deployer Helm Chart].
____

[arabic]
. Login to the SUSE Application Collection:

[source,console]
----
# Replace REPLACE_WITH_YOUR_USERNAME and REPLACE_WITH_YOUR_PASSWORD with your actual credentials
helm registry login dp.apps.rancher.io/charts -u <REPLACE_WITH_YOUR_USERNAME@apps.rancher.io> -p REPLACE_WITH_YOUR_PASSWORD
----

[arabic]
. Create a namespace for SUSE AI

[source,console]
----
kubectl create namespace suse-ai
----

[arabic]
. Create a Kubernetes Pull Secret (`application-collection`) for the SUSE Application Collection:

[source,console]
----
# Replace REPLACE_WITH_YOUR_USERNAME and REPLACE_WITH_YOUR_PASSWORD with your actual credentials
kubectl create -n suse-ai secret docker-registry application-collection --docker-server=dp.apps.rancher.io --docker-username=<REPLACE_WITH_YOUR_USERNAME@apps.rancher.io> --docker-password=REPLACE_WITH_YOUR_PASSWORD
----

==== Install Open Web UI

This section describes how to install the Open Web UI, which provides a user-friendly interface for interacting with AI models.

[arabic]
. Create a link:https://github.com/trento-project/mcp-server/blob/main/examples/values.openwebui.yaml[values.openwebui.yaml] file with the values for Open Web UI.

[arabic]
. Install the Open Web UI using Helm and the values file (`values.openwebui.yaml`) created above. Note that you need to have `cert-manager` properly installed. Besides, an ingress controller is required. If you don't have any, you might need to tweak up the Kubernetes services.

[source,console]
----
helm -n suse-ai upgrade --install open-webui oci://dp.apps.rancher.io/charts/open-webui -f values.openwebui.yaml
----

==== Deploying a model with Ollama remotely

If your infrastructure does not have enough resources to run Ollama models, you can deploy them on a remote server (e.g., Google Cloud Run).

This section describes how to deploy the https://ollama.com/library/qwen3:8b[`Qwen3:8b`] model using Ollama in Google Cloud Run. This guide assumes you have a Google Cloud account and the necessary permissions to deploy applications on Google Cloud Run.

____
NOTE: Always refer to the https://cloud.google.com/run/docs/tutorials/gpu-gemma-with-ollama[official Google documentation] for the most accurate and up-to-date information.
____

[arabic]
. Create a link:https://github.com/trento-project/mcp-server/blob/main/Dockerfile[Dockerfile] for the Ollama model.

[arabic]
. In the same directory as the `Dockerfile`, run the following command to build the Docker image:

[source,console]
----
gcloud run deploy qwen3-8b --source . --concurrency 4 --cpu 8 --set-env-vars OLLAMA_NUM_PARALLEL=4 --gpu 1 --gpu-type nvidia-l4 --max-instances 1 --memory 32Gi --no-allow-unauthenticated --no-cpu-throttling --no-gpu-zonal-redundancy --timeout=600
----

____
NOTE: for making requests you need either to allow unauthenticated requests or use a service account with the necessary permissions. Follow the https://cloud.google.com/run/docs/authenticating/service-to-service[Google Cloud Run documentation] for more information on how to set up service accounts and permissions.
____

As a result, you will have a Google Cloud Run service running the `Qwen3:8b` model, which can be accessed via the URL provided by Google Cloud Run. It might look like `https://qwen3-8b-++<++project-id++>++.++<++region++>++-.run.app`.

==== Adding the remote model to Open Web UI

Once you have the Ollama model running on Google Cloud Run, you can add it to the Open Web UI for easy access.

[arabic]
. Go to Open Web UI settings page (eg. `suse-ai.example.com/admin/settings`) and navigate to the "`Connections`" section.
. Add a new Ollama connection with the URL of your deployed model (eg. `https://qwen3-8b-++<++project-id++>++.++<++region++>++-.run.app`).
. Navigate to the "`Models`" section and click on "`Manage Models`", select the Ollama connection you just created, and pull the `Qwen3:8b` model.

==== Deploying the MCP Server Trento

To deploy the `Trento MCP Server` using SUSE AI, you can use the provided Helm chart. This section describes how to deploy the MCP Server Trento with the necessary configurations.

[arabic]
. Create a link:https://github.com/trento-project/mcp-server/blob/main/examples/values.mcpo.yaml[values.mcpo.yaml] file with the values for the MCP Server Trento:

[arabic]
. Install the MCP Server Trento using Helm and the values file (`values.mcpo.yaml`) created above:

[source,console]
----
helm -n suse-ai upgrade --install trento-for-suse-ai ./helm/trento-mcp-server --values values.mcpo.yaml
----

==== Adding the MCP Server Trento to Open Web UI

The MCP Server Trento can be added to the Open Web UI in two ways: as a user-defined connection or as a pre-configured connection for the model. The former needs a publicly accessible URL (the HTTP call happens in the user's browser), while the latter can be used with a Kubernetes service (the HTTP call happens in the Open Web UI backend). In this guide, we will use the pre-configured connection.

[arabic]
. Go to Open Web UI settings page (eg. `suse-ai.example.com/admin/settings`) and navigate to the "`Tools`" section.
. Click on "`Add connection`" and add the FQDN internal URL for the Kubernetes service (eg. `http://trento-for-suse-ai-trento-mcp-server-mcpo.suse-ai.svc.cluster.local/trento`).
. Navigate to the "`Models`" section and click on "`Manage Models`", select the MCP Server Trento tool for the model. This will enable the MCP Server for Trento to be used with the model by any user in the Open Web UI.
. Optionally, you can tweak the model configuration. For instance:
[arabic]
.. System prompt: `/no++_++think You are focused on solving issues in SAP Systems. You will use the tools. Trento has several endpoints to discover SAP Systems, including HANA clusters and information about the hosts. Refer to the tools whenever possible. Never switch to Chinese, always English.`
.. Context Length (Ollama): 8000
